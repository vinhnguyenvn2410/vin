{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bits of tips from here and there while i'm trying to learn the \"proper\" or the \"better\" ways to analyze datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1 Questioning the Question\n",
    "\"\"\"“What kinds of data are available?”; \n",
    "“What action might result in answering this question?”; “What resources are available to do this analysis?”; .\n",
    "“Is it possible to collect new data?”\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8d5b4e177db2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#EDA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#null check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#describe data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#EDA\n",
    "\n",
    "#missing data function\n",
    "def missing_data(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "\n",
    "\n",
    "#null check\n",
    "data.isnull().sum()\n",
    "\n",
    "#describe data\n",
    "data.describe()\n",
    "#nunique \n",
    "data.describe(include =['O'])\n",
    "data.nunique()\n",
    "\n",
    "#crosstab for stats\n",
    "pd.crosstab(df_train.Pclass, df_train.Survived, margins=True).style.background_gradient(cmap='autumn_r')\n",
    "pd.crosstab(df.A,df.B, normalize='index')\n",
    "\n",
    "#seaborn hist graph comparing a var with true outcomes/classification\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "g = sns.FacetGrid(data, col = 'Attrition_cat')\n",
    "g.map(plt.hist,'JobSatisfaction', bins = 5)\n",
    "\n",
    "#other way to do plot above\n",
    "sns.catplot(x='Sex', col='Survived', kind='count', data=df_train);\n",
    "#for the catplot, you can switch out var like below for a different take \n",
    "sns.catplot(x='Survived', col='Embarked', kind='count', data=df_train);\n",
    "#or\n",
    "sns.catplot('Embarked','Survived', hue= 'Sex', kind='point', data=df_train)\n",
    "plt.show()\n",
    "\n",
    "#and get the %\n",
    "print(\"% of women survived: \" , df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\n",
    "print(\"% of men survived:   \" , df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n",
    "\n",
    "#pairplots\n",
    "col = ['Attrition', 'Age', 'MonthlyIncome','JobLevel','DistanceFromHome']\n",
    "sns.pairplot(data[col], kind = 'reg', diag_kind = 'kde', hue = 'Attrition')\n",
    "\n",
    "#.cat.codes (similar to get dummies)\n",
    "data[\"Attrition_cat\"] = data[\"Attrition\"].cat.codes\n",
    "#I used cat.codes to add a new column called attrition_cat as below. Cool!\n",
    "\n",
    "###countplot This could lead to either under sampling or not since we can find unbalance-ness in train and test datasets\n",
    "sns.countplot(x='Survived', data=df_train);\n",
    "sns.countplot(x='Survived', data=df_test);\n",
    "\n",
    "#% of ppl survived in train dataset.\n",
    "print(df_train.AMI_FLAG.sum()/df_test.AMI_FLAG.count())\n",
    "\n",
    "#group_by categories \n",
    "df_train.groupby(['Survived','Sex'])['Survived'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Wrangling\n",
    "#correlation maxtrix and heatmap visualization\n",
    "corr = df_train_ml.corr()\n",
    "\n",
    "f,ax = plt.subplots(figsize=(9,6))\n",
    "sns.heatmap(corr, annot = True, linewidths=1.5 , fmt = '.2f',ax=ax)\n",
    "plt.show()\n",
    "\n",
    "#get dummies, drop columns, dropnas\n",
    "df_train_ml = pd.get_dummies(df_train_ml, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\n",
    "df_train_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age_bin', 'Fare_bin'],axis=1,inplace=True)\n",
    "df_train_ml.dropna(inplace=True)\n",
    "\n",
    "#Dealing with missing data:\n",
    "1. StringIO(), isnull() to find null data. dropna()-axis, how, thresh, subset to get rid of NaN\n",
    "2. Interpolation: Imputer using mean, median or most_frequent. sklearn.preprocessing/Imputer\n",
    "3. Categorical data: mapping or encode class labels. Or easier: pd.get_dummies\n",
    "\n",
    "\n",
    "#Partitioning Data:\n",
    "1. sklearn.crosslidation import train_test_split (refer to classification exercise for example). Assigns test data size need balancing.\n",
    "\n",
    "#Standardize the features (putting em on the same scale):\n",
    "1. Normalization: min-max scaling (x - min / range). \n",
    "sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "x_train_norm = mms.fit_transform(X_Train)\n",
    "X_test_norm = mms.transform(X_Test)\n",
    "\n",
    "#2. Standardization: statistical method to find Z\n",
    "code similar to MinMaxScaler.\n",
    "\n",
    "Select Meaningful features: some ways to prevent overfitting: collect more data, intro penalty for complexity via regularization, \n",
    "simpler model, reduce attributes/dimensionality of data\n",
    "Penalty regularization: L1 and L2 sklearn.linear LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-bb98acbd6b36>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-bb98acbd6b36>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    page 120 Seb's book on Sequential Backward Selection to help you visualize and choose best features.\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Feature selection: Selection and Extraction\n",
    "page 120 Seb's book on Sequential Backward Selection to help you visualize and choose best features.\n",
    "PAge 124 will talk about using random forest to calculate and visualize feature importance (no need standardized or normalized)\n",
    "\n",
    "#Correlation analysis between continuous variable and target variable:\n",
    "#correlation maxtrix and heatmap visualization\n",
    "corr = df_train_ml.corr()\n",
    "\n",
    "f,ax = plt.subplots(figsize=(9,6))\n",
    "sns.heatmap(corr, annot = True, linewidths=1.5 , fmt = '.2f',ax=ax)\n",
    "plt.show()\n",
    "\n",
    "#Feature selection: analyzing categorical variables vs target categorical variable with Chi Square test:\n",
    "#keep in mind: Null hypothesis Ho = NO correlation between var and target categorical var.\n",
    "class ChiSquare:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "        self.p = None #P-Value\n",
    "        self.chi2 = None #Chi Test Statistic\n",
    "        self.dof = None\n",
    "        \n",
    "        self.dfObserved = None\n",
    "        self.dfExpected = None\n",
    "        \n",
    "    def _print_chisquare_result(self, colX, alpha):\n",
    "        result = \"\"\n",
    "        if self.p<alpha:\n",
    "            result=\"{0} is IMPORTANT for Prediction\".format(colX)\n",
    "        else:\n",
    "            result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(colX)\n",
    "\n",
    "        print(result)\n",
    "        \n",
    "    def TestIndependence(self,colX,colY, alpha=0.05):\n",
    "        X = self.df[colX].astype(str)\n",
    "        Y = self.df[colY].astype(str)\n",
    "        \n",
    "        self.dfObserved = pd.crosstab(Y,X) \n",
    "        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)\n",
    "        self.p = p\n",
    "        self.chi2 = chi2\n",
    "        self.dof = dof \n",
    "        \n",
    "        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n",
    "        \n",
    "        self._print_chisquare_result(colX,alpha)\n",
    "#Initialize ChiSquare Class\n",
    "cT = ChiSquare(df)\n",
    "\n",
    "#Feature Selection\n",
    "testColumns = ['ORIG_REAS_ENTITLE_CD','Education_level','Online_purchaser','Est_BMI_decile']\n",
    "for var in testColumns:\n",
    "    cT.TestIndependence(colX=var,colY=\"AMI_FLAG\")\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From loading to submission:\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "PATH = \"Oren/Kaggle/Housing Prices/\"  #where you put the files\n",
    "df_train = pd.read_csv(f'{PATH}train.csv', index_col='Id')\n",
    "df_test = pd.read_csv(f'{PATH}test.csv', index_col='Id')\n",
    "target = df_train['SalePrice']  #target variable\n",
    "df_train = df_train.drop('SalePrice', axis=1)\n",
    "df_train['training_set'] = True\n",
    "df_test['training_set'] = False\n",
    "df_full = pd.concat([df_train, df_test])\n",
    "df_full = df_full.interpolate()\n",
    "df_full = pd.get_dummies(df_full)\n",
    "df_train = df_full[df_full['training_set']==True]\n",
    "df_train = df_train.drop('training_set', axis=1)\n",
    "df_test = df_full[df_full['training_set']==False]\n",
    "df_test = df_test.drop('training_set', axis=1)\n",
    "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "rf.fit(df_train, target)\n",
    "preds = rf.predict(df_test)\n",
    "my_submission = pd.DataFrame({'Id': df_test.index, 'SalePrice': preds})\n",
    "my_submission.to_csv(f'{PATH}submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

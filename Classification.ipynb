{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will learn how to implement a classifier that can predict if a new customer will likely default on his credit card payment. This is a *supervised* machine learning task as we try to learn a function that maps customer characteristics to credit default behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset which we will be using for this section describes the credit defaulting behavior for our existing customers. Load this data using *pandas'* `read_csv(file_path, index_col=0)` function and assign the result to a new variable `df`. In that statement, replace `file_path` with the file path for the *Default.csv* file on your computer, e.g., *\"data/Default.csv\"*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data into Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's pandas library provides powerful functionalities to load data from different sources, including *CSV files*, *Excel files*, and various *database systems*. Once loaded, the data is represented as a `DataFrame`. You can think of a `DataFrame` as an Excel spreadsheet or an SQL table.\n",
    "\n",
    "[Help: Reading Data From Different Sources](http://pandas.pydata.org/pandas-docs/stable/io.html)\n",
    "\n",
    "We can use *pandas'* `head()` command to look at the first few rows of the data we loaded just now. Type `df.head()` into the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display first few rows of our dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Question**: How do we display all entries with a balance of less than 500? (Hint: df.query(\"column_name < value\") applies a filter to our dataset)\n",
    "\n",
    "In addition, it is always a good idea to test the dimensionality of the imported data using the `shape` command. This is important to make sure you have really imported all the data and imported it in the correct way. The `shape` command produces two values; the first is the number of rows, the is second the number of columns.\n",
    "\n",
    "Enter `df.shape` into the following cell to see if the data was loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check dimensionality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can construct our classification model, we need to do a little bit of *data preprocessing*. We can see that the *student* and *default* attributes in our dataset hold text instead of numeric values. This can be problematic for Python and scikit-learn. Fortunately, we can use `df.replace(old_value, new_value)` to easily replace values in our dataset.\n",
    "\n",
    "Using `df.replace(old_value, new_value)`, replace all occurrences of *\"Yes\"* with the value *1* and all *\"No\"* occurrences with *0*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert strings to numerical values and reassign to 'df' variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Question**: How do we check if the String values have been replaced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Dataset Into Training & Test Subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate classification performance later on, we want to split our dataset into random training and test subsets. This means that we reserve a few rows in the dataset for evaluation/testing (10% in this case) and use the remaining rows to train our classifier.\n",
    "\n",
    "Fortunately, *scikit-learn* provides a `train_test_split(data_frame, test_size=0.1)` function which does this for us. It returns two subsets of our data; the first is the training subset and the second is the test subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use train_test_split to create training and test sets and assign to variables df_train and df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Question**: How do we display the number of samples in the training and test subsets?\n",
    "\n",
    "[Help: Split Dataset](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html)\n",
    "\n",
    "We also need to separate the data into features (input variables) and target (output variable). We want to predict the target column using the feature columns. \n",
    "\n",
    "In this section, we will use the *student*, *balance*, and *income* attributes to predict the *default* column. Let's define the names of the *feature columns as a list* and *create new variables* for the feature and target data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define feature columns \"student\", \"balance\", and \"income\" as a list\n",
    "\n",
    "\n",
    "# Define training feature and target values\n",
    "\n",
    "\n",
    "# Define test feature and target values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in the right format now and we can construct our classifier using the training subset. We pass both the feature (`X_train`) and the target (`Y_train`) *training* values to the classifier. We say that the classifier will try to *learn a relationship* between the two.\n",
    "\n",
    "In this section, we will use the Gaussian Naive Bayes classifier provided by the scikit-learn library. Naive Bayes classification is simple, yet produces very accurate results on many datasets.\n",
    "\n",
    "Let's train the classifier using the data in our training subsets `X_train` and `Y_train`. Create a classification object with `GaussianNB()` and assign it to a variable called `gnb`. Afterwards, you can learn the model coefficients by calling the `fit` command on the classification object. The `fit` command requires two parameters: `X_train` and `Y_train`. Run the code in the following cell to execute training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Help: Naive Bayes Classifier](http://scikit-learn.org/stable/modules/naive_bayes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the classifier's `predict` command to predict the values for the testing subset. Afterwards, we will compare the predicted classes with the actual classes in our dataset. Note that we only pass the features (`X_test`) and not the labels to the `predict` command.\n",
    "\n",
    "Enter `clf.predict(X_test)` into the following cell and assign the results to a new variable called `Y_pred`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Accuracy\n",
    "One of the most important parts of data science includes assessing the accuracy of our results. There are several methods that we can use to understand the performance of our classifier.\n",
    "\n",
    "### Calculate Plain Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest ways to evaluate our classifier is to count the *number of misclassifications* in our test set.\n",
    "\n",
    "Execute the code in the following cell to display the *number of misclassifications*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (Y_test != Y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Question**: How do we change the code above to display the number of *correct* classifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *confusion matrix* visualizes the performance of our classifier in more detail. Each *column* of the matrix represents the instances in a *predicted* class while each *row* represents the instances in an *actual* class.\n",
    "\n",
    "Use the `confusion_matrix(actual_classes, predicted_classes)` command to display the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Help: Confusion Matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Frequencies\n",
    "\n",
    "From the confusion matrix above, we can see that the classes are not *balanced* in the test set. We should always investigate the distribution of the target classes/labels. *numpy's* `np.bincount(labels)` command does this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.bincount(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily generate a barplot to illustrate this result. This becomes particularly helpful when working with a larger number of classes.\n",
    "\n",
    "Execute the following code to create the bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "barwidth = 0.5\n",
    "index = np.arange(len(np.unique(Y_test)))\n",
    "plt.figure()\n",
    "plt.bar(index, np.bincount(Y_test), barwidth)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(index + 0.5 * barwidth, ('0', '1'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Classification Report\n",
    "\n",
    "*Scikit-learn* provides the convenient `classification_report(actual_classes, predicted_classes)` command which displays a more detailed text report of our classifier's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Help: Generate Classification Report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Receiver Operating Characteristic\n",
    "\n",
    "*Receiver Operating Characteristic (ROC)* is a plot of a binary (two-class) classifier with *false positive rate* on the x-axis against *true positive rate* on the y-axis. A ROC plot depicts relative trade-offs that a classifier makes between benefits (true positives) and costs (false positives). This plot also decouples classifier performance from the conditions under which the classifiers will be used. \n",
    "\n",
    "When a single number is needed to summarize classification performance, we can use the *Area under the ROC curve (AUC)* measure.\n",
    "\n",
    "Execute the following code to plot the ROC curve and AUC value for our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get sample scores for ROC curve\n",
    "Y_score = clf.predict_proba(X_test)\n",
    "\n",
    "# Compute ROC metrics\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, Y_score[:, 1], pos_label=1)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Classification Accuracy Over Cross Validation\n",
    "\n",
    "In order to obtain a better estimate of classification error rates, we employ cross validation and plot the ROC curves for each fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure cross validation\n",
    "cv = StratifiedKFold(n_splits=6)\n",
    "\n",
    "# Create new plot\n",
    "plt.figure()\n",
    "\n",
    "# Get indices for training and test subsets for each fold\n",
    "for i, (train, test) in enumerate(cv.split(df[feature_cols], df.default)):\n",
    "    \n",
    "    # Get training data\n",
    "    X_train = df[feature_cols].as_matrix()[train, :]\n",
    "    Y_train = df.default.as_matrix()[train,]\n",
    "    \n",
    "    # Get test data\n",
    "    X_test = df[feature_cols].as_matrix()[test, :]\n",
    "    Y_test = df.default.as_matrix()[test,]\n",
    "    \n",
    "    # Configure classifier\n",
    "    clf = GaussianNB()\n",
    "    \n",
    "    # Learn classification model\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    # Get sample scores for ROC curve\n",
    "    Y_score = clf.predict_proba(X_test)\n",
    "    \n",
    "    # Compute ROC metrics\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, Y_score[:, 1], pos_label=1)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.plot(false_positive_rate, true_positive_rate, label='AUC = %0.2f'% roc_auc)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Help: Cross Validation](https://www.cs.cmu.edu/~schneide/tut5/node42.html)  \n",
    "[Help: ROC Curve With Cross Validation](http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#example-model-selection-plot-roc-crossval-py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
